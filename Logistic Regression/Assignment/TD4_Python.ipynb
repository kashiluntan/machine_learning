{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD4 Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponding pdf:\n",
    "- ml01_s19_td04.pdf\n",
    "\n",
    "Corresponding R code:\n",
    "- TD4.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to learn those notions by yourself:\n",
    "- ROC curve\n",
    "\n",
    "For Naive Bayes and LDA, just take them as a classifier that does the same thing as Logistic Regression. For this moment, there is no need to dig into their what, how and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Towards the end of your exercise, you should be able to explain what's the use of each line of the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart = pd.read_csv(\"SAheart.data\")\n",
    "heart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCQ 1:\n",
    "\n",
    "### What does this cell below do?\n",
    "\n",
    "Choose all answers that appear correct to you.\n",
    "\n",
    "Please write down your answer for Question 1 to string mcq1 (replace 'X' with your choices, e.g. 'A', 'AB', 'BCD' etc).\n",
    "\n",
    "(A) replace the values of column \"famhist\" with 1 for \"Present\", 0 for \"Absent\", so that the target could be scalar instead of string.\n",
    "\n",
    "(B) insert a new column \"famhist\" into the heart dataframe, so that the target could be scalar instead of string.\n",
    "\n",
    "(C) replace the values of column \"famhist\" with 1 for \"Present\", 0 for \"Absent\", so that the feature could be scalar instead of string.\n",
    "\n",
    "(D) we can remove inplace=True because it's the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.famhist.replace(to_replace=['Present', 'Absent'], value=[1, 0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq = 'X'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ§ª Check your answer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('mcq',\n",
    "    mcq=mcq\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.drop(['row.names'], axis=1, inplace=True)\n",
    "heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(heart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.iloc[:,list(range(5)) + [9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For you to reflect ...\n",
    "\n",
    "Why do ?\n",
    "\n",
    "*Profs/TAs might ask you this question during next class.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(heart.iloc[:, list(range(5)) + [9]], hue=\"chd\", diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(heart.iloc[:, list(range(5, 9)) + [9]], hue=\"chd\", diag_kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart.iloc[:, :-1]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = heart.iloc[:, -1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(['famhist'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(X_train.drop(['famhist'], axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lda.predict(X_test.drop(['famhist'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "print(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_test != y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_lda  = 1 - sum(np.diag(perf)) / float(len(y_test)) \n",
    "err_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda = QDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda.fit(X_train.drop(['famhist'], axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = qda.predict(X_test.drop(['famhist'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "print(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_test != y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_qda  = 1 - sum(np.diag(perf)) / float(len(y_test)) \n",
    "err_qda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "print(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_test != y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_nb = 1 - sum(np.diag(perf)) / float(len(y_test)) \n",
    "err_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here, just one line\n",
    "# Write your code here, just one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "print(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_test != y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_logreg = 1 - sum(np.diag(perf)) / float(len(y_test)) \n",
    "err_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(err_lda, err_qda, err_nb, err_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "labels = ['err_lda', 'err_qda', 'err_nb', 'err_logreg']\n",
    "err_list = [err_lda, err_qda, err_nb, err_logreg]\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35\n",
    "ax.bar(x, err_list, width)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 replications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100\n",
    "ERR = np.zeros((M, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(M):\n",
    "    if((i + 1) % 20 == 0):\n",
    "        print(i + 1)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.33)\n",
    "    \n",
    "    lda = LDA()\n",
    "    lda.fit(X_train.drop(['famhist'], axis=1), y_train)\n",
    "    y_pred = lda.predict(X_test.drop(['famhist'], axis=1))\n",
    "    ERR[i, 0] = np.mean(y_test != y_pred)\n",
    "    \n",
    "    qda = QDA()\n",
    "    qda.fit(X_train.drop(['famhist'], axis=1), y_train)\n",
    "    y_pred = qda.predict(X_test.drop(['famhist'], axis=1))\n",
    "    ERR[i, 1] = np.mean(y_test != y_pred)\n",
    "    \n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_pred = nb.predict(X_test)\n",
    "    ERR[i, 2] = np.mean(y_test != y_pred)\n",
    "    \n",
    "    logreg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    # Write your code here, just one line\n",
    "    # Write your code here, just one line\n",
    "    ERR[i, 3] = np.mean(y_test != y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Test error rate')\n",
    "ax.set_ylabel('Test error rate')\n",
    "ax.boxplot(ERR)\n",
    "labels = ['err_lda', 'err_qda', 'err_nb', 'err_logreg']\n",
    "ax.set_xticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "## Plot of ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "lda.fit(X_train.drop(['famhist'], axis=1), y_train)\n",
    "preds = lda.predict_proba(X_test.drop(['famhist'], axis=1))[:,1]\n",
    "fpr, tpr, threshold = sklearn.metrics.roc_curve(y_test, preds)\n",
    "plt.plot(fpr, tpr, 'black', label='lda')\n",
    "\n",
    "qda = QDA()\n",
    "qda.fit(X_train.drop(['famhist'], axis=1), y_train)\n",
    "preds = qda.predict_proba(X_test.drop(['famhist'], axis=1))[:,1]\n",
    "fpr, tpr, threshold = sklearn.metrics.roc_curve(y_test, preds)\n",
    "plt.plot(fpr, tpr, 'red', label='qda')\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "preds = nb.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, threshold = sklearn.metrics.roc_curve(y_test, preds)\n",
    "plt.plot(fpr, tpr, 'blue', label='nb')\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# Write your code here, just one line\n",
    "preds = logreg.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, threshold = sklearn.metrics.roc_curve(y_test, preds)\n",
    "plt.plot(fpr, tpr, 'green', label='logreg')\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'y--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Part II: Vowel data \n",
    "\n",
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel = pd.read_csv(\"vowel.data\", header=None, delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel.rename(columns={10: 'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vowel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['aqua', 'black', 'blue', 'coral', 'brown', \n",
    "           'fuchsia', 'green', 'indigo', 'plum', 'purple', 'navy']\n",
    "sns.pairplot(vowel.iloc[:, list(range(5)) + [10]], hue=\"class\", palette=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(vowel.iloc[:, list(range(5, 10)) + [10]], hue=\"class\", palette=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = vowel.shape[1]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vowel.iloc[:, :-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = vowel.iloc[:, -1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "lda.fit(X_train, y_train)\n",
    "y_pred = lda.predict(X_test)\n",
    "perf = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "print(perf)\n",
    "err_lda = np.mean(y_test != y_pred)\n",
    "assert(err_lda == 1 - sum(np.diag(perf)) / float(len(y_test)))\n",
    "print(err_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda = QDA()\n",
    "qda.fit(X_train, y_train)\n",
    "y_pred = qda.predict(X_test)\n",
    "perf = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "print(perf)\n",
    "err_qda = np.mean(y_test != y_pred)\n",
    "print(err_qda)\n",
    "print(1 - sum(np.diag(perf)) / float(len(y_test)))\n",
    "assert(np.round(err_qda, 5) == \n",
    "       np.round(1 - sum(np.diag(perf)) / float(len(y_test)), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "perf = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "print(perf)\n",
    "err_nb = np.mean(y_test != y_pred)\n",
    "print(err_nb)\n",
    "print(1 - sum(np.diag(perf)) / float(len(y_test)))\n",
    "assert(np.round(err_nb, 5) == \n",
    "       np.round(1 - sum(np.diag(perf)) / float(len(y_test)), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Write your code here, just one line\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Write your code here, just one line\n",
    "\n",
    "print(perf)\n",
    "\n",
    "# Write your code here, just one line\n",
    "\n",
    "print(err_logreg)\n",
    "print(1 - sum(np.diag(perf)) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For you to reflect ...\n",
    "\n",
    "Why would this assertion in the following cell be correct all the time?\n",
    "\n",
    "*Profs/TAs might ask you this question during next class.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.round(err_logreg, 5) == \n",
    "       np.round(1 - sum(np.diag(perf)) / float(len(y_test)), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array([err_lda,err_qda,err_nb,err_logreg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10\n",
    "ERR = np.zeros((M, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(M):\n",
    "    if((i + 1) % 2 == 0):\n",
    "        print(i + 1)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.33)\n",
    "    \n",
    "    lda = LDA()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "    ERR[i, 0] = np.mean(y_test != y_pred)\n",
    "    \n",
    "    qda = QDA()\n",
    "    qda.fit(X_train, y_train)\n",
    "    y_pred = qda.predict(X_test)\n",
    "    ERR[i, 1] = np.mean(y_test != y_pred)\n",
    "    \n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_pred = nb.predict(X_test)\n",
    "    ERR[i, 2] = np.mean(y_test != y_pred)\n",
    "    \n",
    "    logreg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    \n",
    "    # Write your code here, just one line\n",
    "    \n",
    "    # Write your code here, just one line\n",
    "    \n",
    "    ERR[i, 3] = np.mean(y_test != y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Test error rate')\n",
    "ax.set_ylabel('Test error rate')\n",
    "ax.boxplot(ERR)\n",
    "labels = ['err_lda', 'err_qda', 'err_nb', 'err_logreg']\n",
    "ax.set_xticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No test for your code\n",
    "There is no test for your code. Profs/TAs will check your code in person."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
